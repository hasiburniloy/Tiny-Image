{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "from os import getcwd\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function\n",
    "#...............................................................................\n",
    "#data extraction\n",
    "def data_extract(zip_f_path,extract_path,Flag=False):\n",
    "    \n",
    "    \n",
    "    #zip_f_path contain the zipfile location\n",
    "    #extract_path contain the extracting location\n",
    "    Extract=Flag\n",
    "    if Extract:\n",
    "        \n",
    "        \n",
    "        local_zip =zip_f_path \n",
    "        zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "        zip_ref.extractall(extract_path)\n",
    "        zip_ref.close()\n",
    "    else:\n",
    "        print(\"File already downloaded\")\n",
    "\n",
    "#...............................................................................\n",
    "\n",
    "#Rename folder\n",
    "def frenam(mapfilep, fpath):\n",
    "    \n",
    "    # mapfilep is the path of word file to map name\n",
    "    # fpath is the path of the folder \n",
    "   \n",
    "    text = open(mapfilep, 'r')\n",
    "    lines = text.readlines()\n",
    "    names = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        k = line.split('\\t')[0]\n",
    "        v = line.split('\\t')[1].split(',')[0].replace('\\n', '')\n",
    "\n",
    "        names[k] = v\n",
    "    _dir = fpath\n",
    "    folders = os.listdir(_dir)\n",
    "    try:\n",
    "        for fn in folders:\n",
    "            if os.path.isdir(os.path.join(_dir, fn)):\n",
    "                os.rename(os.path.join(_dir, fn), os.path.join(_dir, names[fn]))\n",
    "        return print(\"Complete\")\n",
    "    except:\n",
    "        return print('Already done')\n",
    "#...............................................................................\n",
    "\n",
    "#Validation set processing\n",
    "\n",
    "def val_process(source, dest, txtfpath):\n",
    "    ## making dataframe to group file name based on label of folder\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(txtfpath, sep=\" \", header=None)\n",
    "    for i in range(10000):\n",
    "        df.loc[i, 'val1'] = df.iloc[i, 0].split(\"\\t\")[0]\n",
    "        df.loc[i, 'val2'] = df.iloc[i, 0].split(\"\\t\")[1]\n",
    "    a = df.drop([0], axis=1).groupby(['val2', 'val1']).size().to_frame()\n",
    "\n",
    "    ##Converting df to dictionary\n",
    "    dic = {}\n",
    "    li = []\n",
    "    for i in range(len(a.index)):\n",
    "        if i == len(a.index) - 1:\n",
    "            li.append(a.index[i][1])\n",
    "            dic[a.index[i][0]] = li\n",
    "        else:\n",
    "            if a.index[i][0] == a.index[i + 1][0]:\n",
    "                li.append(a.index[i][1])\n",
    "            else:\n",
    "                li.append(a.index[i][1])\n",
    "                dic[a.index[i][0]] = li\n",
    "                li = []\n",
    "    ##making path for destination\n",
    "    if os.path.exists(dest):\n",
    "        print(\"May be your file already processed,if not then delete the folder\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        os.mkdir(dest)\n",
    "        ##copying file from source to destination\n",
    "        for k, l in dic.items():\n",
    "\n",
    "            try:\n",
    "                shutil.rmtree(dest + k + '/')\n",
    "                os.mkdir(dest + k + '/')\n",
    "                file = l\n",
    "                for li_valu in file:\n",
    "\n",
    "                    for f in os.listdir(source):\n",
    "\n",
    "                        if li_valu == f:\n",
    "                            desfile = dest + k + '/' + f\n",
    "                            thisfile = source + f\n",
    "                            copyfile(thisfile, desfile)\n",
    "\n",
    "            except:\n",
    "\n",
    "                os.mkdir(dest + k + '/')\n",
    "                file = l\n",
    "                for li_valu in file:\n",
    "\n",
    "                    for f in os.listdir(source):\n",
    "\n",
    "                        if li_valu == f:\n",
    "                            desfile = dest + k + '/' + f\n",
    "                            thisfile = source + '/' + f\n",
    "                            copyfile(thisfile, desfile)\n",
    "\n",
    "        print(\"Completed\")\n",
    "#...............................................................................\n",
    "\n",
    "# Mdodel building\n",
    "def tiny_model(epoch,tbatch,vbatch,trainpath,valpath):\n",
    "    \n",
    "  \n",
    "    #epoch is the number of iteration\n",
    "    #tbatch train batch size\n",
    "    #vbatch validation batch size\n",
    "    #trainpath training data path\n",
    "    #valpath validation data path\n",
    "\n",
    "    #model \n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64,(3,3), input_shape=(64, 64, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(512,(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(200, activation='softmax')\n",
    "  ])\n",
    "    model.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "  \n",
    "  # label generation \n",
    "    TRAINING_DIR = trainpath\n",
    "    train_datagen = ImageDataGenerator(\n",
    "      rescale=1 / 255,\n",
    "  )\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "      TRAINING_DIR,\n",
    "      batch_size=tbatch,\n",
    "      class_mode='categorical',\n",
    "      target_size=(64, 64)\n",
    "        )\n",
    "\n",
    "    VALIDATION_DIR =valpath\n",
    "    validation_datagen = ImageDataGenerator(\n",
    "       rescale=1 / 255\n",
    "  )\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "      VALIDATION_DIR,\n",
    "      batch_size=vbatch,\n",
    "      class_mode='categorical',\n",
    "      target_size=(64,64)\n",
    "  )\n",
    "    model.summary()\n",
    "    history = model.fit_generator(train_generator,\n",
    "                              epochs=epoch,\n",
    "                              verbose=1,steps_per_epoch=100000/tbatch,\n",
    "                              validation_data=validation_generator)\n",
    "    return history\n",
    "#.............................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded\n"
     ]
    }
   ],
   "source": [
    "#extracting data\n",
    "zip_f_path='/content/drive/MyDrive/task/archive.zip'\n",
    "extract_path='/content/drive/MyDrive/task/dataset'\n",
    "data_extract(zip_f_path,extract_path,Flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already done\n",
      "May be your file already processed,if not then delete the folder\n",
      "Already done\n"
     ]
    }
   ],
   "source": [
    "## folder rename of training set\n",
    "mp=\"tiny-imagenet-200/words.txt\"\n",
    "fpath=\"tiny-imagenet-200/train\"\n",
    "names=frenam(mp,fpath)\n",
    "## validation processing\n",
    "source='tiny-imagenet-200/val/images/'\n",
    "dest='tiny-imagenet-200/newval/'\n",
    "txtfpath='tiny-imagenet-200/val/val_annotations.txt'\n",
    "val_process(source,dest,txtfpath)\n",
    "##rename folder of validation\n",
    "mp=\"tiny-imagenet-200/words.txt\"\n",
    "fpath=dest\n",
    "frenam(mp,fpath) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 10000 images belonging to 200 classes.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 62, 62, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 29, 29, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               102600    \n",
      "=================================================================\n",
      "Total params: 2,702,664\n",
      "Trainable params: 2,702,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "782/781 [==============================] - 454s 581ms/step - loss: 4.8275 - acc: 0.0460 - val_loss: 4.1805 - val_acc: 0.1148\n",
      "Epoch 2/8\n",
      "782/781 [==============================] - 452s 577ms/step - loss: 4.0226 - acc: 0.1447 - val_loss: 3.7633 - val_acc: 0.1850\n",
      "Epoch 3/8\n",
      "782/781 [==============================] - 440s 562ms/step - loss: 3.5756 - acc: 0.2141 - val_loss: 3.5743 - val_acc: 0.2174\n",
      "Epoch 4/8\n",
      "782/781 [==============================] - 441s 564ms/step - loss: 3.2415 - acc: 0.2709 - val_loss: 3.2453 - val_acc: 0.2721\n",
      "Epoch 5/8\n",
      "782/781 [==============================] - 443s 567ms/step - loss: 2.9579 - acc: 0.3179 - val_loss: 3.3277 - val_acc: 0.2662\n",
      "Epoch 6/8\n",
      "782/781 [==============================] - 443s 566ms/step - loss: 2.7038 - acc: 0.3648 - val_loss: 3.1620 - val_acc: 0.2956\n",
      "Epoch 7/8\n",
      "782/781 [==============================] - 434s 555ms/step - loss: 2.4529 - acc: 0.4093 - val_loss: 3.1691 - val_acc: 0.2978\n",
      "Epoch 8/8\n",
      "781/781 [============================>.] - ETA: 0s - loss: 2.2265 - acc: 0.4524"
     ]
    }
   ],
   "source": [
    "epoch=8\n",
    "tbatch=128\n",
    "vbatch=32\n",
    "trainpath=\"tiny-imagenet-200/train\"\n",
    "valpath='tiny-imagenet-200/newval/'\n",
    "\n",
    "history=tiny_model(epoch,tbatch,vbatch,trainpath,valpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "\n",
    "\n",
    "plt.title('Training and validation loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
